{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This programming assignment has several sections for each part you should fill out the the jupyter notebook. I suggest \"saving as\" a new filename so if you accidentally delete some of the instructions while editing you can recover easily.    If that happens you can also \"rename\" the file and when you reload the jupyter hub it will restore any missing files from my repository state. \n",
    "\n",
    "In the code excercises below there are places containing \"...\" you should replace those without removing the rest of what is present. \n",
    "\n",
    "e.g.: \n",
    "\n",
    "The ... jumped over the ...\n",
    "\n",
    "The cow jumped over the moon\n",
    "\n",
    "\n",
    "You will submit the work to MMS by uploading your .ipynb file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **PS4016 Assignment 1 Rubric** | **Markdown/Narrative Content**                                                                                                                                       | **Code Comment**                                                 | **Code Content**                                                                                                                   |\n",
    "| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **1st**           | Coherent and complete description of motivating questions,  rationale for analysis, and correct description of conclusions to be drawn from resulting data analysis. | Code comments are present and correct for all non-trivial code.  | Complete, working and correct code in all parts assignment.                                                                        |\n",
    "| **2.1**           | Coherent explanations of most parts project with minor errors and/or missing key/important details.                                                                  | Code comments or sometimes missing and/or contain minor errors.  | Code executes without errors (warnings OK) but in some cases produces unexpected/incorrect results.                                |\n",
    "| **2.2**           | Partially incoherent/unclear answer with occasional substantive errors in understanding.                                                                             | Code comments mostly missing with occasional substantive errors. | Code does not execute and fails with errors, but code would take minor revision to correct.                                        |\n",
    "| **3rd**           | Unclear/uncoherent answer that addresses only a minoirty of the assignment                                                                                           | Code comments not present.                                       | Code does not execute and would take major revisions to correct, multiple substantive errors and/or mostly irrelevant information. |\n",
    "| **Fail**          | No narrative descriptions present                                                                                                                                    |                                                                  | Non-executing code, completely incorrect and/or irrelevant to question.                                                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Excercises\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Combining lists and functions and logic.  \n",
    "\n",
    "Combnining various skills from what what you created above to create a program counts **down** in steps of 3: 51, 48, 45, ..., and stops right before numbers become negative. \n",
    "\n",
    "If the value is -even- print the string \"even\", if it's odd print the number.  \n",
    "\n",
    "Your function should be able to work with any integer starting value.  Show it's output for starting from 9 and 32.\n",
    "\n",
    "For 9 you should get something like the following  \n",
    "coutdown(9)  \n",
    "9  \n",
    "even  \n",
    "3  \n",
    "\n",
    "\n",
    "It's also could be on one line:\n",
    "9, even, 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check if a number is even.  return true if even, and false if odd\n",
    "def isEven(...):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "def countdown(...):\n",
    "\n",
    "    ...\n",
    "\n",
    "    for ... in ...:\n",
    "        ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code\n",
    "\n",
    "Assignment Section: Debugging Data Processing Functions\n",
    "\n",
    "Introduction\n",
    "In this part of the assignment, you will work on debugging code. Each set is meant to manipulate or transform data in some way but contains errors or bugs that you need to identify and correct. Each set of code will be associated with specific requirements and expected outputs. Your job is to carefully analyze, test, and debug each one of them.\n",
    "\n",
    "I want you to report: \n",
    "A) Design Tests: Determine some tests to run and explain the rationale for your tests and what they will test.   \n",
    "B) Execute the test: Based on your strategy, write and run tests\n",
    "C) Analyze and Explain the Results: After testing, analyze and explain what you found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First function to test\n",
    "\n",
    "THis function is supposed to create an output that contains the running sum of a column:\n",
    "\n",
    "Input:  1, 2, 3, 4\n",
    "Intended Output: 1, 3, 6, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def cumulative_sum(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate the cumulative sum of a specified column in a DataFrame,\n",
    "    excluding the first element.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame.\n",
    "    column_name (str): The name of the column to calculate the cumulative sum.\n",
    "    \n",
    "    Returns:\n",
    "    Series: A Series containing the cumulative sum, with the first element as zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    cum_sum = [0]  # Initializing cumulative sum with 0\n",
    "    for i in range(1, len(df[column_name]) ):  \n",
    "        cum_sum.append(cum_sum[i-1] + df[column_name][i])\n",
    "        \n",
    "    return pd.Series(cum_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd function to test\n",
    "\n",
    "The function, merge_and_sum_normalized, is constructed to normalize specified columns in two data frames, merge the frames based on a common ID, and compute the sum of normalized values, storing the result in a new column.\n",
    "\n",
    "This function is supposed to \"normalize\" which transforms the data into the new range 0 -1. That means the minimum gets moved to 0, and the maximum gets moved to 1 and everything in between is rescaled.  It's a common step in data analysis.  For example if you calculate a correlation between 2 things the data is first normalized onto the range 0-1.\n",
    "  \n",
    "\n",
    "Example:\n",
    "Values: [100, 200, 300]\n",
    "normalized values: [0. , 0.5, 1. ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_and_sum_normalized(df1, df2, id_column=\"ID\", col1=\"Value1\", col2=\"Value2\"):\n",
    "    \"\"\"\n",
    "    This function takes in two dataframes. It normalizes the values in the specified columns,\n",
    "    merges the dataframes on a common ID, and calculates the sum of the normalized values.\n",
    "    \n",
    "    Parameters:\n",
    "    df1 (DataFrame): The first dataframe.\n",
    "    df2 (DataFrame): The second dataframe.\n",
    "    id_column (str): The name of the ID column for merging.\n",
    "    col1 (str): The name of the column in df1 to be normalized and summed.\n",
    "    col2 (str): The name of the column in df2 to be normalized and summed.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A merged dataframe with a new column representing the sum of normalized values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalizing the specified columns in both dataframes\n",
    "    df1[col1] = (df1[col1] - df1[col1].min()) / (df1[col1].max() - df1[col1].min())\n",
    "    df2[col2] = (df2[col2] - df2[col2].min()) / (df2[col2].max() - df2[col2].min())\n",
    "    \n",
    "    # Merging the two dataframes on the ID column\n",
    "    merged_df = pd.merge(df1, df2, on=id_column, how='inner')\n",
    "    \n",
    "    # Calculating the sum of the normalized columns\n",
    "    merged_df['Sum_Normalized'] = merged_df[col1] + merged_df[col2]\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "# Example of way to create sample data frames for testing\n",
    "df1 = pd.DataFrame({\n",
    "    'ID': ...,\n",
    "    'Value1': ...\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'ID': ...,\n",
    "    'Value2': ...\n",
    "})\n",
    "\n",
    "# Calling the function with the sample data frames\n",
    "result = merge_and_sum_normalized(df1, df2)\n",
    "\n",
    "# Displaying the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid in Scotland\n",
    "\n",
    "This section is a mini real world data analysis.  There is lots of open data available from various sources. In this excercise I want you to take real public data about COVID cases in Scotland and combine it with demographic information. \n",
    "\n",
    "## Covid Data\n",
    "The COVID data comes from Public Health Scotland: https://www.publichealthscotland.scot/our-areas-of-work/covid-19/covid-19-data-and-intelligence/covid-19-daily-cases-in-scotland-dashboard/overview-of-the-daily-covid-19-data-dashboard/\n",
    "\n",
    "The COVID data is the file: scotland_covid_by_IZ.csv\n",
    "\n",
    "I have done various things to make the dataset a bit easier to handle.  I've reduced the data to providing just 1 value per week instead of every day. I've also created some extra columns for ease of use.   \n",
    "\n",
    "The covid data includes (among others) the following columns that you should find useful:\n",
    "\n",
    "| Column Name| Description|\n",
    "|-----------|-------------|\n",
    "| Date| Data string YYYYMMDD|\n",
    "|IntZone|The unique ID code for the intermediate zone the data comes from **USE FOR GROUPING**|\n",
    "|IntZoneName| The name for the zone **NOT UNIQUE DO NOT USE FOR GROUPING**|\n",
    "|CA| Code indentifying the council area|\n",
    "|CAName| Council name|\n",
    "|Positive7Day|  Number of postive cases in the preceding 7 days|\n",
    "|Population| Population of intermediate zone, based on the latest population estimate figures.|\n",
    "|Days from 2020-03-01| Number of days from March 1st, 2020, use for plotting|\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "## Demographic Data\n",
    "The demographic data about student populations comes from the Scotland 2011 census table #QS102SC. I preprocessed the raw data to provide an easier analysis for this assignment.   The census data is given in terms of \"Data Zones\" which are small areas of about 500-1000 people.  But because COVID status is potentially sensitive information in order to protect privacy COVID data is released at the \"Intermediate Zone\" (IZ) level.  These are groups of about about 5 data zones.  So when using this data it is needed to aggregagate the demographic datazones into intermediate zone level to be able to combine the demographic information with the covid dataset.  **I have done this for you already.**\n",
    "\n",
    "The student population data is in the file: studentPop_by_IZ.csv\n",
    "\n",
    "| Column Name| Description|\n",
    "|-----------|-------------|\n",
    "|InterZone|The intermediate zone unique ID|\n",
    "|studentPop|Number of students in the zone|\n",
    "\n",
    "The Scottish Index of Multiple Deprivation (SIMD) is the method Scotland uses to quantify Socie-Economic Status of local areas of scotland. More information can be found from the the Scottish Government website [link](https://www.gov.scot/collections/scottish-index-of-multiple-deprivation-2020/).  \n",
    "Commonly SIMD is grouped into 5 \"quintiles\" from 1(most deprived) to 5 (least deprived) at a level of postcodes.  There is no direct equivalent for intermediate zones. There are various ways to quantify the SIMD of the intermediate zones (IZ).  I've provided a a version for you that looks at the population within an IZ and classifies it by the largest SIMD quintile by population. \n",
    "\n",
    "The SIMD data is in the file: scotland_simd_by_IZ.csv\n",
    "| Column Name| Description|\n",
    "|-----------|-------------|\n",
    "|InterZone|The intermediate zone unique ID|\n",
    "|All people| The population for the intermediate zone, **Note: This is from a different census measure than above, so may differ slightly**\n",
    "|Area (hectares)| The area of the zone in hectares|\n",
    "|simd_mode| The simd with the largest population|\n",
    "\n",
    "\n",
    "# Tasks\n",
    "\n",
    "\n",
    "1) A table of the Scottish Councils for overall total covid cases per 100,000 population. Which includes the Council Name, the total cases/100k, total population in council, and population density in people per Hectare\n",
    "\n",
    "2) A table of the top 10 Scottish Intermediate Zones for highest daily incidence of covid cases per 100,000 population. Which includes the IZ Name, Council Name, the covid incidence in cases/100k, the date this occured, the student population percentage, the level of deprivation as indexed by SIMD.\n",
    "\n",
    "3) A table of the top 10 Scottish Intermediate Zones for cumulative total covid cases per 100,000 population. Which includes the IZ Name, Council Name, the total cases/100k, the student population percentage, the level of deprivation as indexed by SIMD. \n",
    "\n",
    "4) Graph the cases/100k overtime for the Intermediate Zone with the highest daily incidence. Note: We haven't covered how to handle dates.  So I created a column \"Days from 2020-03-01\" in the dataset.  Use this for the x axis.  In order to make the graph(s) you will need to make a dataframe with appropriate columns. Make sure you calculate the values explicitly and correctly. \n",
    "\n",
    "4) Graph cases/100k overtime seperating intermediate zones with students making up 20% or more of the population and zones with less than 20% of the population being students. Note: We haven't covered how to handle dates.  So I created a column \"Days from 2020-03-01\" in the dataset.  Use this for the x axis.  In order to make the graph(s) you will need to make a dataframe with appropriate columns. Make sure you calculate the values explicitly and correctly. \n",
    "\n",
    "5)  The Welly Ball.  In 2021 there was a report that a charity ball at St Andrews was the cause of a spike in cases. See: https://www.thecourier.co.uk/fp/news/fife/2734368/charity-ball-linked-to-spike-in-covid-cases-at-st-andrews-university/  Can you provide some data analysis to support or refute this claim?   For this task I'm not giving you an explicit output to create. Use your judgement to create an analysis. \n",
    "You can find an interactive map of the intermediate zones in Fife here: \n",
    "https://statistics.gov.scot/atlas/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fid%2Fstatistical-geography%2FS12000047\n",
    "\n",
    "\n",
    "\n",
    "**NOTE** When you have that you can use whatever way to make the graph you want. Depending on how you structure you dataframes different ways work easier than others.  One way I suggest is using the seaborn lineplot function in the dataset and use the following function (substiting the appropriate values in the inputs for the variable containing your data and the columns names you want to plot): \n",
    "```\n",
    "sns.lineplot(data=DATAFRAMENAME,x=COLUMN_NAME,y=COLUMN_NAME,hue=COLUMN_NAME)\n",
    "```\n",
    "However, beware that this can make plots you don't intend. Be sure to calculate correcly and organize your data clearly so you know what to plot.  \n",
    "\n",
    "It also may be useful and easier to understand if convert from days from 2020-3-01 to weeks from 2020-3-01.  Just divide days by 7 to get this value. \n",
    "\n",
    "### For each task you need to do: \n",
    "\n",
    "A) Write code to generate the output\n",
    "\n",
    "B) Include written description of your code.  This should be written inline with the code using a # comment line\n",
    "```\n",
    "#Divide cases by the population to get per capita rate and multipy by 100,000 to get cases per 100k\n",
    "result = 100,000*(cases/population)\n",
    "```\n",
    "\n",
    "C) Finish wiht a markdown cell that explains what the output shows and how you interpret this output. \n",
    "\n",
    "### Note:\n",
    "\n",
    "You will need to create some derived outcome measures and combine data to effectively do these tasks.  You'll need to calculate population density and student population percentage, and covid incidence. You may also need other derived columns depending on your strategy for creating the outputs. \n",
    "The covid incidence might be new to you.  There are various ways to look at covid outcomes. A common metric used is cases per 100,000 population. This normalizes by population so it's not always areas with bigger populations that are higher (8,000 cases in St Andrews is very different to 8,000 cases in London).  To get this value you calculate case per person and then multiply by 100,000. This can be calculated with the following formula.\n",
    "\n",
    "```\n",
    "result = 100,000*(cases/population)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read in the covid case data from the files. \n",
    "covidData = pd.read_csv('scotland_covid_by_IZ.csv')\n",
    "\n",
    "#read in the SIMD data\n",
    "simdData = pd.read_csv('scotland_simd_by_IZ.csv')\n",
    "\n",
    "#read in the student population data\n",
    "studentPop = pd.read_csv('studentPop_by_IZ.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmaPy38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
